---
title: "Clase 3"
author: "Mario Matus"
date: "July 05, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE, warning = FALSE)
```

# Modelo de Regresión Básico
* Mínimos cuadrados es una herramienta de estimación.
* Para realizar inferencia se desarrolla un modelo probabilístico de regresión lineal
$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_{i}
$$
* Aquí $\varepsilon_{i}$ se asume iid $N(0, \sigma^2)$. 
* Note que $E[Y_i ~|~ X_i = x_i] = \mu_i = \beta_0 + \beta_1 x_i$
* Note que $Var(Y_i ~|~ X_i = x_i) = \sigma^2$.
* La estimación por ML de $\beta_0$ y $\beta_1$ coincide con la estimación por OLS
  $$\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X$$
* $E[Y ~|~ X = x] = \beta_0 + \beta_1 x$
* $Var(Y ~|~ X = x) = \sigma^2$

# Interpretación de los coeficientes

## Intercepto
* $\beta_0$ es el valor esperado del output cuando el input es 0
$$
E[Y | X = 0] =  \beta_0 + \beta_1 \times 0 = \beta_0
$$
* Note que esto no siempre es de interÃ©s, por ejemplo cuando $X=0$ es imposible o está fuera del rango de los datos (e.g. Si $X$ corresponde a presión sanguínea, estatura, etc.)
* Considere que
$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
= \beta_0 + a \beta_1 + \beta_1 (X_i - a) + \varepsilon_i
= \tilde \beta_0 + \beta_1 (X_i - a) + \varepsilon_i
$$
Entonces, si desplazamos $X$ en $a$ unidades cambia el intercepto pero no la pendiente.  menudo $a$ se fija en $\bar X$ tal que  el intercepto se interpreta como la respuesta esperada en el valor promedio de $X$.

## Pendiente

* $\beta_1$ es el cambio esperado en el output cuando el input cambia en una unidad
$$
E[Y ~|~ X = x+1] - E[Y ~|~ X = x] =
\beta_0 + \beta_1 (x + 1) - (\beta_0 + \beta_1 x ) = \beta_1
$$
* Considere el impacto de cambiar las unidades (medición) de $X$
$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
= \beta_0 + \frac{\beta_1}{a} (X_i a) + \varepsilon_i
= \beta_0 + \tilde \beta_1 (X_i a) + \varepsilon_i
$$
* Entonces, la multiplicación de $X$ por un factor $a$ resulta en que se divide el coeficiente por el mismo factor $a$.
* Si queremos predecir el output dado un valor del input, digamos $X$, el modelo de regresión predice
  $$
  \hat \beta_0 + \hat \beta_1 X
  $$

## Ejemplo

* Si $X$ es la estatura en $m$ e $Y$ es el peso en $kg$. Entonces $\beta_1$ es $kg/m$. Convirtiendo $X$ en $cm$ implica multiplicar $X$ por $100 cm/m$. Para obtener $\beta_1$ en las unidades correctas, tenemos que dividir por $100 cm /m$ y así se tendrán las unidades correctas. 
$$
X m \times \frac{100cm}{m} = (100 X) cm
~~\mbox{y}~~
\beta_1 \frac{kg}{m} \times\frac{1 m}{100cm} = 
\left(\frac{\beta_1}{100}\right)\frac{kg}{cm}
$$

# Modelo Lineal Univariado

## Los Datos de Galton

Francis Galton (1882 - 1911) sentó las bases de la Econometría estudiando la estatura de padres e hijos. Los datos de su estudio están disponibles en la instalación de R. El gráfico de estatura de padres versus estatura de hijos es el siguiente:

```{r, fig.height=4, fig.width=5, echo=FALSE}
library(UsingR); data(galton);
library(plyr) #comandos Ãºtiles para bases de datos
library(dplyr)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey60", aes(size = freq, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "royalblue", high="lightblue")                    
g
```

## Ejercicios
* Usar la estatura de los padres para predecir la de los hijos
* Encontrar una relación entre ambas estaturas
* Encontrar la variación de la estatura de los hijos que no depende de la estatura de los padres (variación residual)
* Los supuestos que se necesitan para generalizar más allá de los datos
* Por quÃ© los hijos de padres muy altos tienden a ser más bajos (regresión a la media)

## Análisis de los datos

* Datos recolectados y analizados por Francis Galton en 1885. 
* Galton fue un científico que creó los conceptos de correlación y regresión.
* Veremos la distribución marginal de los datos.
* La corrección por gÃ©nero se obtiene multiplicando la estatura de las mujeres por 1,08.

```{r galton,fig.height=3.5,fig.width=8}
library(reshape); long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable)) 
g <- g + geom_histogram(colour = "black", binwidth=1) 
g <- g + facet_grid(. ~ variable)
g
```

## Encontrando la media usando mínimos cuadrados

* Veamos un poco los datos
```{r}
head(galton, n=10) #primeras 10 observaciones (medidas en pulgadas)
dim(galton) #tamaÃ±o muestral = 928 ; variables  = 2
```
* Considere solo la estatura de los hijos. Â¿Cómo se describe la media?
* Una definición es que siendo $Y_i$ la estatura del hijo $i$ para $i= 1,\ldots , n$ con $n=928$entonces la media es el valor de $\mu$ que minimiza la ecuación $\sum_{i=1}^n (Y_i - \mu)^2$
* Se tiene que $\mu = \bar Y$.

## Experimento: Usar RStudio para manipular el valor de $\mu$ y observar cuál es el valor que minimiza la ecuación $\sum_{i=1}^n (Y_i - \mu)^2$

Código a replicar en R
```
library(UsingR); data(galton)
library(manipulate)
myHist <- function(mu){
    mse <- mean((galton$child - mu)^2)
    g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "royalblue", colour = "black", binwidth=1)
    g <- g + geom_vline(xintercept = mu, size = 2, colour = "red")
    g <- g + ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
    g
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
```

## El estimador de mínimos cuadrados es la media empírica
```{r , fig.height=4, fig.width=4, fig.align='center'}
g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "royalblue", colour = "black", binwidth=1)
g <- g + geom_vline(xintercept = mean(galton$child), size = 2, colour = "red")
g
```

## Comparando la estatura de los padres e hijos

El tamaÃ±o de los puntos en $(X,Y)$ representa la cantidad de observaciones que coinciden en dicho valor.

```{r, fig.height=6,fig.width=7,echo=FALSE}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey60", aes(size = freq, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "royalblue", high="lightblue")                    
g
```

## Regresión desde el origen

* $X_i$ es la estatura de los padres.
* Se tiene un valor $\beta$ que minimiza la ecuación $\sum_{i=1}^n (Y_i - X_i \beta)^2$
* Se tiene que el intercepto es el punto $(0,0)$ y la recta es tal que minimiza la suma de las distancias verticales al cuadrado desde los puntos de cada observación a la recta.
* Para que el intercepto sea interpretable se debe restar la media de los datos de manera que el intercepto sea $(\bar{X},\bar{Y})$

## Experimento: Usar RStudio para manipular el valor de $\beta$ y observar cuál es el valor que minimiza la ecuación $\sum_{i=1}^n (Y_i - X_i \beta)^2$

```
library(UsingR); data(galton)
library(dplyr)
library(manipulate)
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
myPlot <- function(beta){
  g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
  g <- g  + scale_size(range = c(2, 20), guide = "none" )
  g <- g + geom_point(colour="grey50", aes(size = freq, show_guide = FALSE))
  g <- g + geom_point(aes(colour=freq, size = freq))
  g <- g + scale_colour_gradient(low = "royalblue", high="lightblue")                     
  g <- g + geom_abline(intercept = 0, slope = beta, size = 3)
  mse <- mean( (y - beta * x) ^2 )
  g <- g + ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
  g
}
manipulate(myPlot(beta), beta = slider(0, 1.2, step = 0.05))
```
## Regresión

```{r}
lm(I(child - mean(child)) ~ I(parent - mean(parent)) - 1, data = galton)
```

```{r, fig.height=6,fig.width=7}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey60", aes(size = freq, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "royalblue", high="lightblue")                      
lm1 <- lm(galton$child ~ galton$parent)
g <- g + geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], size = 3, colour = grey(.5))
g
```

## Ajuste de la mejor recta de regresión

* Sea $Y_i$ la estatura del hijo $i^{th}$ y $X_i$ la estatura del padre $i^{th}$. 
* Considere la recta con mejor ajuste $Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$.
* La ecuación de mínimos cuadrados es $$\sum_{i=1}^n [Y_i - (\beta_0 + \beta_1 X_i)]^2.$$

## Resultados

* El modelo de mínimos cuadrados ajusta la recta $Y = \beta_0 + \beta_1 X$ a travÃ©s de los pares ordenados $(X_i, Y_i)$ e $Y_i$ es el output que se obtiene de la recta $Y = \hat \beta_0 + \hat \beta_1 X$ con
  $$\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X.$$
* $\hat \beta_1$ se expresa en unidades de $Y / X$, $\hat \beta_0$ se expresa en unidades de $Y$.
* La recta de regresión pasa por $(\bar X, \bar Y)$.
* La pendiente de la recta de regresión con $X$ como output e $Y$ como input es $Cor(Y, X) \frac{Sd(X)}{Sd(Y)}$. 
* La pendiente es la misma que se obtiene que si se centraran los datos $(X_i - \bar X, Y_i - \bar Y)$ y se estimara una regresión que pasa por $(0,0)$.
* Si se normalizan los datos $\left(\displaystyle\frac{X_i - \bar X}{Sd(X)}, \displaystyle\frac{Y_i - \bar Y}{Sd(Y)} \right)$, la pendiente es $Cor(Y, X)$.

## En síntesis

Es interesante notar que las estimaciones del programa coinciden con el cálculo siguiendo las fórmulas por definición.

```{r, fig.height=4, fig.width=5, echo=TRUE}
y <- galton$child
x <- galton$parent
beta1 <- cor(y, x) *  sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
rbind(c(beta0, beta1), coef(lm(y ~ x)))
```

La regresión desde el origen conserva la pendiente si primero centramos los datos

```{r, fig.height=4,fig.width=4}
yc <- y - mean(y)
xc <- x - mean(x)
beta1 <- sum(yc * xc) / sum(xc ^ 2)
c(beta1, coef(lm(y ~ x))[2])
```

Si se normalizan los datos la pendiente es igual al coeficiente de correlación

```{r, echo=TRUE}
yn <- (y - mean(y))/sd(y)
xn <- (x - mean(x))/sd(x)
c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2])
```

Mejor recta de regresión:

```{r, fig.height=4, fig.width=5, echo=FALSE}
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey60", aes(size = freq, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "royalblue", high="lightblue")                    
g <- g + geom_smooth(method="lm", formula=y~x)
g
```

# Modelo Lineal Multivariado

## Extensión del caso univariado

* El modelo lineal generalizado extiende el modelo lineal simple (SLR) agregando tÃ©rminos linealmente al modelo. Típicamente $X_{1i}=1$ (se incluye un intercepto).
$$
Y_i =  \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots +
\beta_{p} X_{pi} + \epsilon_{i} 
= \sum_{k=1}^p X_{ik} \beta_j + \varepsilon_{i}.
$$
* La estimación por OLS (y tambiÃ©n la estimación por ML bajo supuestos de iid y errores Gaussianos) minimiza
$$
\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2.
$$
* Lo importante es la linealidad de los coeficientes, entonces
$$
Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots +
\beta_{p} X_{pi}^2 + \varepsilon_{i}. 
$$
tambiÃ©n es un modelo lineal (aunque los regresores sean tÃ©rminos cuadráticos).

## Interpretación de los coeficientes

$$E[Y | X_1 = x_1, \ldots, X_p = x_p] = \sum_{k=1}^p x_{k} \beta_k$$

$$
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p] = (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k
$$

$$
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p]  - E[Y | X_1 = x_1, \ldots, X_p = x_p]$$
$$= (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k + \sum_{k=1}^p x_{k} \beta_k = \beta_1 $$
Un coeficiente de regresión multivariada es el cambio esperado en el output ante un cambio en una unidad en el regresor correspondiente, manteniendo todos los demás regresores fijos.

# Tasas de hambre en la población infantil

Instancia de trabajo
```{r}
#link descarga 
url <- "http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv"
file <- "hunger.csv"

if(!file.exists(file)) {
  print("descargando")
  download.file(url, file, method="curl")
}

hunger <- read.csv("hunger.csv")
hunger <- hunger[hunger$Sex!="Both sexes",]
```

Sin controlar por gÃ©nero:

```{r, fig.height=4, fig.width=5, echo=FALSE}
lm1 <- lm(hunger$Numeric ~ hunger$Year)
plot(hunger$Year,hunger$Numeric,pch=20,col="blue")
```

Controlando por gÃ©nero (azul = niÃ±as, verde = niÃ±os):

```{r, fig.height=4, fig.width=5, echo=FALSE}
plot(hunger$Year,hunger$Numeric,pch=20)
#azul=niÃ±as verde=niÃ±os
points(hunger$Year,hunger$Numeric,pch=20,col=((hunger$Sex=="Male")*1+3))
```

## Modelo univariado

Sin controlar por gÃ©nero:

$$Hu_i = b_0 + b_1 Y_i + e_i$$

$b_0$ = % de hambre en el aÃ±o 0

$b_1$ = disminución del % de hambre por aÃ±o

$e_i$ = todas las variables no medidas

```{r, fig.height=4, fig.width=5, echo=FALSE}
lm1 <- lm(hunger$Numeric ~ hunger$Year)
plot(hunger$Year,hunger$Numeric,pch=20,col="blue")
lines(hunger$Year,lm1$fitted,lwd=3,col="red")
```

Controlando por gÃ©nero:

$$HuF_i = bf_0 + bf_1 YF_i + ef_i$$

$bf_0$ = % de hambre en las niÃ±as en el aÃ±o 0

$bf_1$ = disminución del % de hambre por aÃ±o en las niÃ±as

$ef_i$ = todas las variables no medidas


$$HuM_i = bm_0 + bm_1 YM_i + em_i$$

$bm_0$ = % de hambre en los niÃ±os en el aÃ±o 0

$bm_1$ = disminución del % de hambre por aÃ±o en las niÃ±os

$em_i$ = todas las variables no medidas

```{r, fig.height=3.5,fig.width=5,fig.height=4, echo=FALSE}
lmM <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])
lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"])
plot(hunger$Year,hunger$Numeric,pch=20)
points(hunger$Year,hunger$Numeric,pch=20,col=((hunger$Sex=="Male")*1+3))
lines(hunger$Year[hunger$Sex=="Male"],lmM$fitted,col="green",lwd=3)
lines(hunger$Year[hunger$Sex=="Female"],lmF$fitted,col="blue",lwd=3)
```

## Modelo multivariado

Las dos rectas anteriores tienen la misma pendiente. Vamos a estimar el siguiente modelo:

$$Hu_i = b_0 + b_1 M_i + b_2 Y_i + e^*_i$$

$b_0$ = % de hambre en las niÃ±as en el aÃ±o 0

$M_i$ = $\begin{cases} 1 &\text{si es niÃ±o} \cr 0 &\text{si es niÃ±a} \end{cases}$

$b_0 + b_1$ = % de hambre en las niÃ±os en el aÃ±o 0

$b_2$ = disminución del % de hambre por aÃ±o en niÃ±os o niÃ±as

$e^*_i$ = todas las variables no medidas

```{r, fig.height=4, fig.width=5, echo=FALSE}
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex)
plot(hunger$Year,hunger$Numeric,pch=20)
points(hunger$Year,hunger$Numeric,pch=20,col=((hunger$Sex=="Male")*1+3))
abline(c(lmBoth$coeff[1],lmBoth$coeff[2]),col="blue",lwd=3)
abline(c(lmBoth$coeff[1] + lmBoth$coeff[3],lmBoth$coeff[2] ),col="green",lwd=3)
```

$$Hu_i = b_0 + b_1 M_i + b_2 Y_i + b_3 (M_i \cdot Y_i) + e^+_i$$

$b_0$ = % de hambre en las niÃ±as en el aÃ±o 0

$M_i$ = $\begin{cases} 1 &\text{si es niÃ±o} \cr 0 &\text{si es niÃ±a} \end{cases}$

$b_0 + b_1$ = % de hambre en las niÃ±os en el aÃ±o 0

$b_2$ = disminución del % de hambre por aÃ±o en niÃ±os o niÃ±as

$b_2 + b_3$ = disminución del % de hambre por aÃ±o en los niÃ±os

$e^+_i$ = todas las variables no medidas

```{r, fig.height=4, fig.width=5, echo=FALSE}
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Sex*hunger$Year)
plot(hunger$Year,hunger$Numeric,pch=20)
points(hunger$Year,hunger$Numeric,pch=20,col=((hunger$Sex=="Male")*1+3))
abline(c(lmBoth$coeff[1],lmBoth$coeff[2]),col="blue",lwd=3)
abline(c(lmBoth$coeff[1] + lmBoth$coeff[3],lmBoth$coeff[2] +lmBoth$coeff[4]),col="green",lwd=3)
```

## Resultados

```{r, fig.height=4,fig.width=4}
coefficients(lmBoth)
```

# Ejercicio

Usando la encuesta <a href="http://pachamaltese.github.io/analisis-de-datos-unab/laboratorios/laboratorio4/casen2013.dta.zip">CASEN 2013</a> estime un modelo log-lineal
$$ 
\log(y_i) = \beta_0 + \sum_{i=1}^n \beta_i x_i + \varepsilon_i
$$
que permita predecir el salario por hora de los profesionales chilenos de entre 35 y 45 aÃ±os que tienen una jornada laboral de al menos 30 horas por semana.

Considere como regresores las variables:
 
* Sexo
* Experiencia laboral
* Si la persona reside en la Región Metropolitana
* Si la persona trabaja en la Administración PÃºblica

Extienda los resultados de su regresión a la población del país.

# Desarrollo Ejercicio

```{r}
#install.packages("foreign")
#install.packages("httr")
#install.packages("plyr")

library(foreign) #para leer archivos de Stata, SPSS, etc
library(httr) #permite descargar desde la web

###########################
# Diccionario de variables
###########################

# Lo primero es consultar el diccionario de variables donde aparece la explicación de los códigos y la naturaleza de las variables
# http://observatorio.ministeriodesarrollosocial.gob.cl/documentos/Libro_de_Codigos_Casen_2013_Base_Principal_Metodologia_Nueva.pdf

####################################
# Descargar y cargar base de datos
####################################

url <- "http://pachamaltese.github.io/analisis-de-datos-unab/laboratorio4/casen2013.dta.zip"
zip <- "casen2013.dta.zip"
dta <- "casen2013.dta"

if(!file.exists(zip)) {
  print("descargando")
  download.file(url, file, method="curl")
}

if(!file.exists(dta) & file.exists(zip)) {
  unzip(zip)
  casen <- read.dta(dta)
}

if(file.exists(dta)) {
  casen <- read.dta(dta)
}

####################################
# Limpiar datos y generar variables
####################################

# creo una copia que ahorrará tiempo si quiero retroceder algunos pasos
# o sea, si me equivoco vuelvo a ejecutar esta línea y la variable casen sigue intacta (ahorra tiempo)
data <- casen

# conservo en memoria Ãºnicamente las variables relevantes para la regresión
# para esto hay que consultar el diccionario de variables
keep <- c("expr", "yoprcor", "edad", "esc", "sexo", "region", "rama1", "o10")
data <- data[keep]

# veo el tipo de variables en memoria
str(data) #hay variables enteras, factores y numÃ©ricas 

# saco las filas que tengan celdas vacías (o sino habrá que hacer pasos adicionales al final)
data <- na.omit(data)

# hay que crear la variable "logaritmo del salario por hora"
# se consideran los datos de los trabajadores con jornada > 30 horas/semana
# para el cálculo hay que calcular 
# salario ($/mes) * 12 (meses/aÃ±o) / jornada (horas/semana) * 52 (semanas/aÃ±o)
data <- data[data$o10 > 30,]
data$WHP <- (data$yoprcor*12)/(data$o10*52)
data$logWHP <- log(data$WHP)

# conservo solo los datos que corresponden al tramo de edad entre 35 y 45 aÃ±os
data <- data[(data$edad >= 35 & data$edad <= 45),]

# conservo solo los datos que corresponden al nivel de escolaridad pedido
# profesional en Chile = 5 aÃ±os (o más) de educación superior
# entonces Experiencia Laboral = Edad - AÃ±os de Escolaridad ("esc") - 5 (duración mínima de estudios profesionales)
# la ecuación de arriba puede tomar valores negativos ya que hay personas en la encuesta que se encuentran estudiando 
# y trabajan part-time, realizan continuidad de estudios, tienen post-grado, etc. entonces hay que descartar esos casos
data$exp <- data$edad - data$esc - 5
data <- data[data$exp >= 0,]
data$exp2 <- (data$exp)^2

# hay que generar una variable binaria para cada región
# antes de hacer cualquier cosa veo como están asignados los niveles del factor región
levels(data$region)

# los nombres de los niveles contienen espacios, entonces asigno nombres simples
# (ver el excel que usÃ© para hacer que el proceso sea eficiente)
data$region <- revalue(data$region, c("i. tarapaca"="r1",
                                      "ii. antofagasta"="r2",
                                      "iii. atacama"="r3",
                                      "iv. coquimbo"="r4",
                                      "v. valpara\xedso"="r5",
                                      "vi. o higgins"="r6",
                                      "vii. maule"="r7",
                                      "viii. biob\xedo"="r8",
                                      "ix. la araucan\xeda"="r9",
                                      "x. los lagos"="r10",
                                      "xi. ays\xe9n"="r11",
                                      "xii. magallanes"="r12",
                                      "metropolitana"="r13",
                                      "xiv. los r\xedos"="r14",
                                      "xv. arica y parinacota"="r15"))

# verifico que los nombres asignados a los niveles hayan cambiado
levels(data$region)

# para crear las variables binarias uso el comando for de manera que no hay que repetir el comando para cada región
for(i in unique(data$region)) {
  data[paste(i, sep="")] <- ifelse(data$region == i, 1, 0)
}

# hay que generar una variable binaria para cada industria (o rama)
# veo como están asignados los niveles del factor región
levels(data$rama1)

# aparece un nivel que corresponde a "sin clasificar" (nivel X), entonces debo sacar ese nivel y volver a definir los niveles
data<-data[data$rama1 != "x. no bien especificado",]
data$rama1 <- factor(data$rama1) 

# verifico los cambios
levels(data$rama1)

# los nombres de los niveles contienen espacios, entonces asigno nombres simples
data$rama1 <- revalue(data$rama1, c("a. agricultura, ganader\xeda, caza y silvicultura"="sa",
                                    "b. pesca"="sb",
                                    "c. explotaci\xf3n de minas y canteras"="sc",
                                    "d. industrias manufactureras"="sd",
                                    "e. suministro de electricidad, gas y agua"="se",
                                    "f.construcci\xf3n"="sf",
                                    "g. comercio al por mayor y al por menor"="sg",
                                    "h. hoteles y restaurantes"="sh",
                                    "i. transporte, almacenamiento y comunicaciones"="si",
                                    "j. intermediaci\xf3n financiera"="sj",
                                    "k. actividades inmobiliarias, empresariales y de alquiler"="sk",
                                    "l.administrasci\xf3n p\xfablica y defensa"="sl",
                                    "m. ense\xf1anza"="sm",
                                    "n. servicios sociales y de salud"="sn",
                                    "o. otras actividades de servicios comunitarios, sociales y personales"="so",
                                    "p. hogares privados con servicio dom\xe9stico"="sp",
                                    "q.organizaciones y organos extraterritoriales"="sq"))

# verifico los cambios
levels(data$rama1)

# para crear las variables binarias uso el comando for de manera que no hay que repetir el comando para cada región

for(i in unique(data$rama1)) {
  data[paste(i, sep="")] <- ifelse(data$rama1 == i, 1, 0)
}

# si no usaba na.omit al principio hay que usar esto
#drop <- c("NA")
#data <- data[,!(names(data) %in% drop)]
#data <- data[complete.cases(data),]

#############
# Regresiones
#############

# estimadores para la muestra
summary(lm(logWHP ~ sexo + esc + exp + exp2 + r13 + sl, data = data))

# estimadores para la población (población = país)
summary(lm(logWHP ~ sexo + esc + exp + exp2 + r13 + sl, data = data, weights = expr))
```

Resultados: Estimadores para la población (población = país)

```{r, eval=FALSE}
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.5348155  0.0889943   84.67   <2e-16 ***
sexomujer   -0.2591384  0.0096771  -26.78   <2e-16 ***
esc          0.1346419  0.0021693   62.07   <2e-16 ***
exp         -0.1681013  0.0066858  -25.14   <2e-16 ***
exp2         0.0038441  0.0001431   26.86   <2e-16 ***
r13          0.1953442  0.0096526   20.24   <2e-16 ***
sl           0.2222964  0.0204180   10.89   <2e-16 ***

Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1

Residual standard error: 5.793 on 18174 degrees of freedom
Multiple R-squared:  0.371,	Adjusted R-squared:  0.3708 
F-statistic:  1787 on 6 and 18174 DF,  p-value: < 2.2e-16
```
