---
title: "Clase 3"
author: "Pachá"
date: "June 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
library(ggfortify)
```

# Modelo de Regresión Básico
* Mínimos cuadrados es una herramienta de estimación.
* Para realizar inferencia se desarrolla un modelo probabilístico de regresión lineal
$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_{i}
$$
* Aquí $\varepsilon_{i}$ se asume iid $N(0, \sigma^2)$. 
* Note que $E[Y_i ~|~ X_i = x_i] = \mu_i = \beta_0 + \beta_1 x_i$
* Note que $Var(Y_i ~|~ X_i = x_i) = \sigma^2$.
* La estimación por ML de $\beta_0$ y $\beta_1$ coincide con la estimación por OLS
  $$\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X$$
* $E[Y ~|~ X = x] = \beta_0 + \beta_1 x$
* $Var(Y ~|~ X = x) = \sigma^2$

# Interpretación de los coeficientes

## Intercepto
* $\beta_0$ es el valor esperado del output cuando el input es 0
$$
E[Y | X = 0] =  \beta_0 + \beta_1 \times 0 = \beta_0
$$
* Note que esto no siempre es de interés, por ejemplo cuando $X=0$ es imposible o está fuera del rango de los datos (e.g. Si $X$ corresponde a presión sanguínea, estatura, etc.)
* Considere que
$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
= \beta_0 + a \beta_1 + \beta_1 (X_i - a) + \varepsilon_i
= \tilde \beta_0 + \beta_1 (X_i - a) + \varepsilon_i
$$
Entonces, si desplazamos $X$ en $a$ unidades cambia el intercepto pero no la pendiente.  menudo $a$ se fija en $\bar X$ tal que  el intercepto se interpreta como la respuesta esperada en el valor promedio de $X$.

## Pendiente

* $\beta_1$ es el cambio esperado en el output cuando el input cambia en una unidad
$$
E[Y ~|~ X = x+1] - E[Y ~|~ X = x] =
\beta_0 + \beta_1 (x + 1) - (\beta_0 + \beta_1 x ) = \beta_1
$$
* Considere el impacto de cambiar las unidades (medición) de $X$
$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
= \beta_0 + \frac{\beta_1}{a} (X_i a) + \varepsilon_i
= \beta_0 + \tilde \beta_1 (X_i a) + \varepsilon_i
$$
* Entonces, la multiplicación de $X$ por un factor $a$ resulta en que se divide el coeficiente por el mismo factor $a$.
* Si queremos predecir el output dado un valor del input, digamos $X$, el modelo de regresión predice
  $$
  \hat \beta_0 + \hat \beta_1 X
  $$

## Ejemplo

* Si $X$ es la estatura en $m$ e $Y$ es el peso en $kg$. Entonces $\beta_1$ es $kg/m$. Convirtiendo $X$ en $cm$ implica multiplicar $X$ por $100 cm/m$. Para obtener $\beta_1$ en las unidades correctas, tenemos que dividir por $100 cm /m$ y así se tendrán las unidades correctas. 
$$
X m \times \frac{100cm}{m} = (100 X) cm
~~\mbox{y}~~
\beta_1 \frac{kg}{m} \times\frac{1 m}{100cm} = 
\left(\frac{\beta_1}{100}\right)\frac{kg}{cm}
$$

# Modelo Lineal Univariado

## Los Datos de Galton

Francis Galton (1882 - 1911) sentó las bases de la Econometría estudiando la estatura de padres e hijos. Los datos de su estudio están disponibles en la instalación de R. El gráfico de estatura de padres versus estatura de hijos es el siguiente:

```{r galton1, fig.height=4, fig.width=5, echo=FALSE}
library(UsingR); data(galton);
library(dplyr)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey60", aes(size = freq, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "royalblue", high="lightblue")                    
g
```

## Ejercicios
* Usar la estatura de los padres para predecir la de los hijos
* Encontrar una relación entre ambas estaturas
* Encontrar la variación de la estatura de los hijos que no depende de la estatura de los padres (variación residual)
* Los supuestos que se necesitan para generalizar más allá de los datos
* Por qué los hijos de padres muy altos tienden a ser más bajos (regresión a la media)

## Análisis de los datos

* Datos recolectados y analizados por Francis Galton en 1885. 
* Galton fue un científico que creó los conceptos de correlación y regresión.
* Veremos la distribución marginal de los datos.
* La corrección por género se obtiene multiplicando la estatura de las mujeres por 1,08.

```{r galton2,fig.height=3.5,fig.width=8}
library(reshape); long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable)) 
g <- g + geom_histogram(colour = "black", binwidth=1) 
g <- g + facet_grid(. ~ variable)
g
```

## Encontrando la media usando mínimos cuadrados

* Veamos un poco los datos
```{r galton3}
head(galton, n=10) #primeras 10 observaciones (medidas en pulgadas)
dim(galton) #tamaño muestral = 928 ; variables  = 2
```
* Considere solo la estatura de los hijos. ¿Cómo se describe la media?
* Una definición es que siendo $Y_i$ la estatura del hijo $i$ para $i= 1,\ldots, n$ con $n=928$ entonces la media es el valor de $\mu$ que minimiza la ecuación $\sum_{i=1}^n (Y_i - \mu)^2$
* Se tiene que $\mu = \bar Y$.

## Regresión

### Sin constante
$$Y_i = \beta_1 X_i + \varepsilon_i$$
```{r galton6_1}
lm(child ~ parent -1, data = galton)
```

### Con constante
$$Y_i = \beta_0 + \beta_1 X_i  + \varepsilon_i$$
```{r galton6_2}
lm(child ~ parent, data = galton)
```

### Sin constante (centrando los datos)
$$(Y_i - \bar{Y}) = \beta_1 (X_i - \bar{X})  + \varepsilon_i$$
$$\tilde{Y}_i = \beta_0 \beta_1 \tilde{X}_i  + \varepsilon_i$$
```{r galton7_1}
lm(I(child - mean(child)) ~ I(parent - mean(parent)) -1, data = galton)
```

### Con constante (centrando los datos)
$$(Y_i - \bar{Y}) = \beta_0 + \beta_1 (X_i - \bar{X})  + \varepsilon_i$$
$$\tilde{Y}_i = \beta_0 \beta_1 \tilde{X}_i  + \varepsilon_i$$
```{r galton7_2}
lm(I(child - mean(child)) ~ I(parent - mean(parent)) -1, data = galton)
```

### Observaciones

* La primera regresión (sin constante) sobreestima el efecto de $X$ sobre $Y$ pues se fuerza que el intercepto sea cero
* No es coincidente que las últimas tres regresiones entreguen un idéntico valor de $\beta_1$. 
* Lo anterior se debe a que al no incluir el "-1" en el código en R, no estamos forzando que el intercepto sea cero y por ende no estamos sesgando el efecto de $X$ sobre $Y$.
* En las dos últimas regresiones se observa que el intercepto al centrar los datos (es decir, al cambiar la unidad de medición y hacer que el "0" la variable sea el promedio de dicha variable) sea un valor muy pequeño corresponde a un hecho atribuible a los datos y no a que se fuerza la estimación.

###  Gráfico

```{r galton8, fig.height=5,fig.width=5, echo=FALSE}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey60", aes(size = freq, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "royalblue", high="lightblue")                      
fit <- lm(galton$child ~ galton$parent)
g <- g + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 3, colour = grey(.5))
g
```

## Ajuste de la mejor recta de regresión

* Sea $Y_i$ la estatura del hijo $i^{th}$ y $X_i$ la estatura del padre $i^{th}$. 
* Considere la recta con mejor ajuste $Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$.
* La ecuación de mínimos cuadrados es $$\sum_{i=1}^n [Y_i - (\beta_0 + \beta_1 X_i)]^2.$$

## Resultados

* El modelo de mínimos cuadrados ajusta la recta $Y = \beta_0 + \beta_1 X$ a través de los pares ordenados $(X_i, Y_i)$ e $Y_i$ es el output que se obtiene de la recta $Y = \hat \beta_0 + \hat \beta_1 X$ con
  $$\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X.$$
* $\hat \beta_1$ se expresa en unidades de $Y / X$, $\hat \beta_0$ se expresa en unidades de $Y$.
* La recta de regresión pasa por $(\bar X, \bar Y)$.
* La pendiente de la recta de regresión con $X$ como output e $Y$ como input es $Cor(Y, X) \frac{Sd(X)}{Sd(Y)}$. 
* La pendiente es la misma que se obtiene que si se centraran los datos $(X_i - \bar X, Y_i - \bar Y)$ y se estimara una regresión que pasa por $(0,0)$.
* Si se normalizan los datos $\left(\displaystyle\frac{X_i - \bar X}{Sd(X)}, \displaystyle\frac{Y_i - \bar Y}{Sd(Y)} \right)$, la pendiente es $Cor(Y, X)$.

## En síntesis

Es interesante notar que las estimaciones del programa coinciden con el cálculo siguiendo las fórmulas por definición.

```{r galton9, fig.height=4, fig.width=5, echo=TRUE}
y <- galton$child
x <- galton$parent
beta1 <- cor(y, x) *  sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
rbind(c(beta0, beta1), coef(lm(y ~ x)))
```

La regresión desde el origen conserva la pendiente si primero centramos los datos

```{r galton10, fig.height=4,fig.width=4}
yc <- y - mean(y)
xc <- x - mean(x)
beta1 <- sum(yc * xc) / sum(xc ^ 2)
c(beta1, coef(lm(y ~ x))[2])
```

Si se normalizan los datos la pendiente es igual al coeficiente de correlación

```{r galton11, echo=TRUE}
yn <- (y - mean(y))/sd(y)
xn <- (x - mean(x))/sd(x)
c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2])
```

Mejor recta de regresión:

```{r galton12, fig.height=4, fig.width=5, echo=FALSE}
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey60", aes(size = freq, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "royalblue", high="lightblue")                    
g <- g + geom_smooth(method="lm", formula=y~x)
g
```

# Modelo Lineal Multivariado

## Extensión del caso univariado

* El modelo lineal generalizado extiende el modelo lineal simple (SLR) agregando términos linealmente al modelo. Típicamente $X_{1i}=1$ (se incluye un intercepto).
$$
Y_i =  \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots +
\beta_{p} X_{pi} + \epsilon_{i} 
= \sum_{k=1}^p X_{ik} \beta_j + \varepsilon_{i}.
$$
* La estimación por OLS (y también la estimación por ML bajo supuestos de iid y errores Gaussianos) minimiza
$$
\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2.
$$
* Lo importante es la linealidad de los coeficientes, entonces
$$
Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots +
\beta_{p} X_{pi}^2 + \varepsilon_{i}. 
$$
también es un modelo lineal (aunque los regresores sean términos cuadráticos).

## Interpretación de los coeficientes

$$E[Y | X_1 = x_1, \ldots, X_p = x_p] = \sum_{k=1}^p x_{k} \beta_k$$

$$
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p] = (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k
$$

$$
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p]  - E[Y | X_1 = x_1, \ldots, X_p = x_p] = (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k - \sum_{k=1}^p x_{k} \beta_k = \beta_1 $$
Un coeficiente de regresión multivariada es el cambio esperado en el output ante un cambio en una unidad en el regresor correspondiente, manteniendo todos los demás regresores fijos.

# Tasas de hambre en la población infantil

Instancia de trabajo
```{r hambre1}
#link descarga 
url <- "http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv"
file <- "hunger.csv"

if(!file.exists(file)) {
  print("descargando")
  download.file(url, file, method="curl")
}

hunger <- read.csv("hunger.csv")
hunger <- hunger[hunger$Sex!="Both sexes",]
```

## Modelo univariado

### Sin controlar por género

$$H_i = b_0 + b_1 Y_i + e_i$$

$b_0$ = % de hambre en el año 0

$b_1$ = disminución del % de hambre por año

$e_i$ = todas las variables no medidas

```{r hambre4, fig.height=4, fig.width=5}
lm1 <- lm(hunger$Numeric ~ hunger$Year)
summary(lm1)

ggplot(hunger, aes(x=Year, y=Numeric)) + geom_point(shape=21,fill="grey50") + geom_smooth(method=lm, se=FALSE) +
  ggtitle("Tasas de hambre en la población infantil") +
  scale_x_continuous(name = "Año") +
  scale_y_continuous(name = "% de hambre")
```

### Controlando por género

$$HF_i = bf_0 + bf_1 YF_i + ef_i$$

$bf_0$ = % de hambre en las niñas en el año 0

$bf_1$ = disminución del % de hambre por año en las niñas

$ef_i$ = todas las variables no medidas


$$HM_i = bm_0 + bm_1 YM_i + em_i$$

$bm_0$ = % de hambre en los niños en el año 0

$bm_1$ = disminución del % de hambre por año en las niños

$em_i$ = todas las variables no medidas

```{r hambre5, fig.height=3.5,fig.width=5,fig.height=4}
lmM <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])
lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"])

summary(lmM)
summary(lmF)

ggplot(hunger, aes(x=Year, y=Numeric, fill=Sex)) + geom_point(shape=21) + geom_smooth(method=lm, aes(color=Sex), se=FALSE) + 
  ggtitle("Tasas de hambre en la población infantil") +
  scale_x_continuous(name = "Año") +
  scale_y_continuous(name = "% de hambre")
```

## Modelo multivariado

### Especificación 1

$$H_i = b_0 + b_1 M_i + b_2 Y_i + e^*_i$$

$b_0$ = % de hambre en las niñas en el año 0

$M_i$ = $\begin{cases} 1 &\text{si es niño} \cr 0 &\text{si es niña} \end{cases}$

$b_0 + b_1$ = % de hambre en las niños en el año 0

$b_2$ = disminución del % de hambre por año en niños o niñas

$e^*_i$ = todas las variables no medidas

```{r hambre6, fig.height=4, fig.width=5}
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex)
summary(lmBoth)
```

### Especificación 2

$$H_i = b_0 + b_1 M_i + b_2 Y_i + b_3 (M_i \cdot Y_i) + e^+_i$$

$b_0$ = % de hambre en las niñas en el año 0

$M_i$ = $\begin{cases} 1 &\text{si es niño} \cr 0 &\text{si es niña} \end{cases}$

$b_0 + b_1$ = % de hambre en las niños en el año 0

$b_2$ = disminución del % de hambre por año en niños o niñas

$b_2 + b_3$ = disminución del % de hambre por año en los niños

$e^+_i$ = todas las variables no medidas

```{r hambre7, fig.height=4, fig.width=5}
lmBoth2 <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Sex*hunger$Year)
summary(lmBoth2)
```

### Residuos

```{r residuos1}
autoplot(lmBoth, label.size = 3)
autoplot(lmBoth2, label.size = 3)
```

# Ejemplo: Economic Fluctuations in the United States 1921–1941 (Klein)

* El paper de Klein es un paper clásico de la economía keynesiana. 
* Propone un mecanismo de funcionamiento de la economía bastante simple
* Ideal para estudiar regresiones en una primera etapa
* Los datos originales están disponibles en la extensión `sem`

Las variables de la base de datos original son:

* Year: 1921–1941
* C = private consumption expenditure.
* P = profits net of business taxes.
* WP = wage bill of the private sector.
* WG = wage bill of the government sector.
* I = (net) private investment.
* K = stock of (private) capital goods (at the end of the year).
* A = an index of the passage of time, 1931 = zero.
* G = government expenditure plus net exports.
* T = business taxes.
* X = gross national product.

Las ecuaciones que Klein testea son:
$$Ct = \alpha_1 + \alpha_2 P_t + \alpha_3 P_{t-1} + \alpha_4 (WP_t + WG_t) + e_t$$
$$I_t = \beta_1 + \beta_2 P_t + \beta_3 P_{t-1} - \beta_4 K_{t-1} + e_t$$
$$WPt = \gamma_1 + \gamma_2 X_t + \gamma_3 X_{t-1} + \gamma_4 A + e_t$$
$$Pt = X_t - WP_t - T_t$$
$$K_t = K_{t-1} + I_t$$
$$X_t = C_t + I_t + G_t$$

Veamos las variables de la base de datos:
```{r klein0}
str(Klein)
```


En el caso de la ecuación $Ct = \alpha_1 + \alpha_2 P_t + \alpha_3 P_{t-1} + \alpha_4 (WP_t + WG_t)$ lo primero es construir las variables `P.lag` y `X.lag` que no están en los datos.

Luego $Ct = \alpha_1 + \alpha_2 P_t + \alpha_3 P_{t-1} + \alpha_4 (WP_t + WG_t)$ en R es
```
C ~ P + P.lag + I(Wp + Wg)
```

Para estimar los coeficientes de esta ecuación se hará lo ya visto anteriormente
```{r klein1}
Klein$P.lag <- c(NA, Klein$P[-length(Klein)])
Klein$X.lag <- c(NA, Klein$X[-length(Klein)])

summary(lm(C ~ P + P.lag + I(Wp + Wg), data=Klein))
```

Para estimar los coeficientes de $I$ y $Wp$ se procede del mismo modo
```{r klein2}
summary(lm(I ~ P + P.lag + K.lag, data=Klein))
summary(lm(Wp ~ X + X.lag + I(Year - 1931), data=Klein))
```